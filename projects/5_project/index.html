<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Neural Radiance Fields | Cyprian Zander </title> <meta name="author" content="Cyprian Zander"> <meta name="description" content="November 14, 2025"> <meta name="keywords" content="machine-learning, computer-science, berkeley, uc-berkeley, portfolio, research, data-science, software-engineering, academic, publications, artificial-intelligence, deep-learning, neural-networks, python, programming, algorithms, statistics, mathematics, cs-student, undergraduate, research-assistant, github, open-source, technical-writing, academic-website, personal-website, student-portfolio, cs180, cs61a, cs61b, cs70, cs188, cs189, launchpad, berkeley-launchpad, startup, entrepreneurship, technology, innovation, stem, education, university, california, bay-area, silicon-valley"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favico.ico?ba4125972f729e5643234e8f9a65339e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zanderfilet.github.io/projects/5_project/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Cyprian</span> Zander </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/books/">bookshelf </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/photography/">photography </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Neural Radiance Fields</h1> <p class="post-description">November 14, 2025</p> </header> <article> <h3 id="overview">Overview</h3> <p>In this project, I built a pipeline for rendering Neural Radiance Fields from images. First, I calibrated my camera using ArUco markers, estimated poses for a 30–50 images of an object, and combined the intrinsics, poses, and undistorted images into a dataset. I then implemented a NeRF by generating camera rays, sampled points along each ray, encoded them with positional encodings, and used an MLP to predict colors and densities. Finally, I used volume rendering to synthesize new views of the object.</p> <hr> <h3 id="part-0-calibrating-your-camera-and-capturing-a-3d-scan">Part 0: Calibrating Your Camera and Capturing a 3D Scan</h3> <h5 id="01-calibrating-your-camera">0.1: Calibrating Your Camera</h5> <p>I calibrated my camera using ArUco markers, capturing 37 images from different angles. For each image, I collected the marker’s 4x4 corner. The <code class="language-plaintext highlighter-rouge">calibrateCamera</code> function in OpenCV helped compute the camera intrinsics and distortion coefficients, which would be useful for my own object dataset curation later.</p> <p>Below are some sample images.</p> <div class="row"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part0/out_1-480.webp 480w,/assets/img/cs180/p4/part0/out_1-800.webp 800w,/assets/img/cs180/p4/part0/out_1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part0/out_1.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Configuration Set Sample 1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part0/out_2-480.webp 480w,/assets/img/cs180/p4/part0/out_2-800.webp 800w,/assets/img/cs180/p4/part0/out_2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part0/out_2.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Configuration Set Sample 2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part0/out_3-480.webp 480w,/assets/img/cs180/p4/part0/out_3-800.webp 800w,/assets/img/cs180/p4/part0/out_3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part0/out_3.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Configuration Set Sample 3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p class="text-center"></p> <h5 id="02-capturing-a-3d-object-scan">0.2: Capturing a 3D Object Scan</h5> <p>For my target NeRF reconstruction, I captured around 35 images of a LEGO model from different angles at a consistent distance, keeping the same camera and zoom level as the calibration dataset.</p> <div class="row"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part0/1-480.webp 480w,/assets/img/cs180/p4/part0/1-800.webp 800w,/assets/img/cs180/p4/part0/1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part0/1.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Dataset Sample 1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part0/2-480.webp 480w,/assets/img/cs180/p4/part0/2-800.webp 800w,/assets/img/cs180/p4/part0/2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part0/2.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Dataset Sample 2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part0/3-480.webp 480w,/assets/img/cs180/p4/part0/3-800.webp 800w,/assets/img/cs180/p4/part0/3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part0/3.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Dataset Sample 3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p class="text-center"></p> <h5 id="03-estimating-camera-pose">0.3: Estimating Camera Pose</h5> <p>For each image containing the ArUco marker, I solved the Perspective-n-Point (PnP) to estimate the camera pose by matching the detected 2D corner positions with their known 3D coordinates. This returns a rotation vector and translation vector representing transformation between the world and the camera. With the viser library, I visualized all of the poses for each image in the dataset.</p> <div class="row"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part0/render-480.webp 480w,/assets/img/cs180/p4/part0/render-800.webp 800w,/assets/img/cs180/p4/part0/render-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part0/render.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Poses View 1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part0/render2-480.webp 480w,/assets/img/cs180/p4/part0/render2-800.webp 800w,/assets/img/cs180/p4/part0/render2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part0/render2.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Poses View 2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p class="text-center"></p> <h5 id="04-undistorting-images-and-creating-a-dataset">0.4: Undistorting images and creating a dataset</h5> <p>In the final step of the data preparation process, I removed the lens distortion from each image to make sure they match the pinhole camera model that NeRF assumes. To eliminate black borders from undistortion, I computed an undistored camera matrix, cropped the images to the valid ROI, and adjusted the principal point coordinates to account for the crop offset. Finally, I split the undistorted images and their c2w poses into training, validation, and test sets.</p> <h3 id="part-1-fit-a-neural-field-to-a-2d-image">Part 1: Fit a Neural Field to a 2D Image</h3> <p>Before working with 3D Neural Radiance Fields, I first implemented a 2D neural field. For this, I built an MLP that takes normalized 2D pixel coordinates as input and outputs RGB color values. To capture high-frequency details, I applied sinusoidal positional encoding to the input coordinates, expanding them from 2D to higher dimensions: $\gamma(p) = [p, \sin(2^0\pi p), \cos(2^0\pi p), \ldots, \sin(2^{L-1}\pi p), \cos(2^{L-1}\pi p)]$ where $L$ is the maximum frequency level. The network was trained using MSE loss between predicted and ground truth colors, optimized with Adam and evaluated using the Peak Signal to Noise Ratio (PSNR).</p> <p>The neural field consists of a 4-layer MLP of variable width (I tested 64 and 256 channels), ReLU activations between layers, and a final Sigmoid activation. I trained for 1000 iterations with a batch size of 10,000 randomly sampled pixels per iteration, using Adam optimizer with learning rate $0.01$.</p> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part1/mlp_img-480.webp 480w,/assets/img/cs180/p4/part1/mlp_img-800.webp 800w,/assets/img/cs180/p4/part1/mlp_img-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part1/mlp_img.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="2D Neural Field Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Here you can see some examples of the neural field inference across increasing iterations.</p> <div class="row"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part1/animal-480.webp 480w,/assets/img/cs180/p4/part1/animal-800.webp 800w,/assets/img/cs180/p4/part1/animal-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part1/animal.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Original" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part1/animal_0-480.webp 480w,/assets/img/cs180/p4/part1/animal_0-800.webp 800w,/assets/img/cs180/p4/part1/animal_0-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part1/animal_0.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="0 iters" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part1/animal_100-480.webp 480w,/assets/img/cs180/p4/part1/animal_100-800.webp 800w,/assets/img/cs180/p4/part1/animal_100-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part1/animal_100.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="100 iters" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part1/animal_900-480.webp 480w,/assets/img/cs180/p4/part1/animal_900-800.webp 800w,/assets/img/cs180/p4/part1/animal_900-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part1/animal_900.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="900 iters" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p class="text-center">Original, 0 iterations, 100 iterations, 900 iterations.</p> <div class="row"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part1/animal_psnr_curve-480.webp 480w,/assets/img/cs180/p4/part1/animal_psnr_curve-800.webp 800w,/assets/img/cs180/p4/part1/animal_psnr_curve-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part1/animal_psnr_curve.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Animal PSNR" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p class="text-center"></p> <p>Below is a grid displaying fully trained results from tweaking positional encoding frequencies and network widths.</p> <div class="row"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part1/animal_freq2_width64_800-480.webp 480w,/assets/img/cs180/p4/part1/animal_freq2_width64_800-800.webp 800w,/assets/img/cs180/p4/part1/animal_freq2_width64_800-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part1/animal_freq2_width64_800.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="freq 2, width 64" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part1/animal_freq2_width256_800-480.webp 480w,/assets/img/cs180/p4/part1/animal_freq2_width256_800-800.webp 800w,/assets/img/cs180/p4/part1/animal_freq2_width256_800-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part1/animal_freq2_width256_800.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="freq 2, width 256" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p class="text-center">max_freq=2; left, width=64, right width=256</p> <div class="row"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part1/animal_freq10_width64_800-480.webp 480w,/assets/img/cs180/p4/part1/animal_freq10_width64_800-800.webp 800w,/assets/img/cs180/p4/part1/animal_freq10_width64_800-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part1/animal_freq10_width64_800.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="freq 10, width 64" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part1/animal_freq10_width256_800-480.webp 480w,/assets/img/cs180/p4/part1/animal_freq10_width256_800-800.webp 800w,/assets/img/cs180/p4/part1/animal_freq10_width256_800-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part1/animal_freq10_width256_800.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="freq 10, width 256" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p class="text-center">max_freq=10; left, width=64, right width=256</p> <p>With a low frequency encoding, the reconstruction loses high-frequency details and appears blurry. With narrow linear layers (width=64), the model struggles to capture fine details even with high-frequency encoding. The best results come from max_freq=10 with width=256, achieving a sharp reconstruction of the original image.</p> <div class="row"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part1/nerf-480.webp 480w,/assets/img/cs180/p4/part1/nerf-800.webp 800w,/assets/img/cs180/p4/part1/nerf-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part1/nerf.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Original" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part1/nerf_0-480.webp 480w,/assets/img/cs180/p4/part1/nerf_0-800.webp 800w,/assets/img/cs180/p4/part1/nerf_0-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part1/nerf_0.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="0 iters" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part1/nerf_100-480.webp 480w,/assets/img/cs180/p4/part1/nerf_100-800.webp 800w,/assets/img/cs180/p4/part1/nerf_100-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part1/nerf_100.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="100 iters" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part1/nerf_900-480.webp 480w,/assets/img/cs180/p4/part1/nerf_900-800.webp 800w,/assets/img/cs180/p4/part1/nerf_900-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part1/nerf_900.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="900 iters" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p class="text-center">Original, 0 iterations, 100 iterations, 900 iterations. It's NeRF or Nothing.</p> <h3 id="part-2-fit-a-neural-radiance-field-from-multi-view-images">Part 2: Fit a Neural Radiance Field from Multi-view Images</h3> <h5 id="21-create-rays-from-cameras">2.1: Create Rays from Cameras</h5> <p>Moving into 3D space, to render novel views with NeRF, I first needed to generate camera rays for each pixel. I implemented three transformation functions to convert pixel coordinates into 3D rays in world space.</p> <p>First, I implemented <code class="language-plaintext highlighter-rouge">transform(c2w, x_c)</code> to convert points from camera coordinates to world coordinates using the camera-to-world matrix from part 0.1. Given a transformation defined by rotation $R$ and translation $t$, this applies $\mathbf{x}_w = R \mathbf{x}_c + t$.</p> <p>Next, I implemented <code class="language-plaintext highlighter-rouge">pixel_to_camera(K, uv, s)</code> to convert pixel coordinates back to camera space. Given the intrinsic matrix:</p> \[K = \begin{bmatrix} f_x &amp; 0 &amp; o_x \\\\ 0 &amp; f_y &amp; o_y \\\\ 0 &amp; 0 &amp; 1 \end{bmatrix}\] <p>and the projection equation $s \mathbf{u} = K \mathbf{x}_c$, I inverted this to get:</p> \[\mathbf{x}_c = \begin{bmatrix} \frac{s(u - o_x)}{f_x} \\\\ \frac{s(v - o_y)}{f_y} \\\\ s \end{bmatrix}\] <p>Finally, I combined these in <code class="language-plaintext highlighter-rouge">pixel_to_ray(K, c2w, uv)</code> to generate rays. For each pixel, the ray origin is simply the camera position $\mathbf{o} = c2w[:3, 3]$. To find the ray direction, I computed a point at depth $s=1$ in camera space, transformed it to world space, and normalized the direction: $\mathbf{d} = \frac{\mathbf{x}_w - \mathbf{o}}{|\mathbf{x}_w - \mathbf{o}|}$.</p> <h5 id="22-sampling">2.2: Sampling</h5> <p>For training, I had to sample rays from multiple images and discretize each ray into 3D sample points. I implemented <code class="language-plaintext highlighter-rouge">sample_rays(images, Ks, c2ws, N)</code> to select $N$ rays from all pixels across all images. I created a pixel grid for all images, added 0.5 to convert from image coordinates to pixel centers, then globally sampled ray indices. For each sampled ray, I computed its origin and direction using the corresponding camera’s intrinsics and extrinsics, along with the ground truth pixel color.</p> <p>To sample points along each ray, I implemented <code class="language-plaintext highlighter-rouge">sample_points_along_rays(origins, directions, near, far, n_samples)</code> which discretizes rays between near and far planes (set to 2.0 and 6.0 for the lego scene). Rather than using uniform samples $t = \text{linspace}(\text{near}, \text{far}, n)$ which may cause overfitting, I introduced stratified sampling with perturbations during training. I divided the ray into equal intervals, then randomly sampled within each interval to ensure every location along the ray gets visited during training. The final 3D coordinates are computed as $\mathbf{p} = \mathbf{o} + t \mathbf{d}$ for each sample distance $t$.</p> <h5 id="23-putting-the-dataloading-all-together">2.3: Putting the Dataloading All Together</h5> <p>Finally, I combined the ray generation and sampling functions into a <code class="language-plaintext highlighter-rouge">RaysData</code> class that precomputes all rays for the training images. The dataloader stores pixel coordinates, calculates ray origins and directions for every pixel across all images, and maps them to their corresponding ground truth colors. During training, the <code class="language-plaintext highlighter-rouge">sample_rays(N)</code> method randomly selects $N$ rays along with their colors for each batch. I verified the implementation using viser to visualize the camera frustums, sampled rays, and 3D sample points, confirming that rays correctly travel from camera positions and sample points lie along the expected ray paths.</p> <p>Below is the viser visualization for an initial Lego wheel loader dataset.</p> <div class="row"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part2/render-480.webp 480w,/assets/img/cs180/p4/part2/render-800.webp 800w,/assets/img/cs180/p4/part2/render-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part2/render.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Wheel Loader Viser" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h5 id="24-neural-radiance-field">2.4: Neural Radiance Field</h5> <p>With the data preparation ready, I implemented the NeRF MLP to predict RGB color and volume density for 3D points. As an extension to the 2D neural field, this network takes both the 3D world coordinates and viewing directions as inputs, since color in a radiance field intuitively depends on view angle. I applied positional encoding to both inputs, using higher frequency for positions ($L=10$) to capture fine geometric details and a lower frequency for directions ($L=4$), which ensured that viewing-dependent effects would be smoother.</p> <p>In this extended model, the architecture consists of eight fully connected layers with ReLU activations. After the first 4 layers, I concatenated the original positionally-encoded coordinates back into the network as a skip connection, which helps retain spatial information. The network then splits into two components. First, there’s a density head that outputs a single positive value (using ReLU), and a color head that takes the intermediate features concatenated with the encoded viewing direction to produce RGB values (with Sigmoid activations to bound outputs to [0,1]).</p> <div class="row"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part2/mlp_nerf-480.webp 480w,/assets/img/cs180/p4/part2/mlp_nerf-800.webp 800w,/assets/img/cs180/p4/part2/mlp_nerf-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part2/mlp_nerf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="NeRF Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>For my own dataset’s model, I scaled the layers to 512.</p> <h5 id="25-volume-rendering">2.5: Volume Rendering</h5> <p>With the NeRF network predicting colors and densities at sampled 3D points, I implemented volume rendering to composite these samples into final pixel colors. The continuous volume rendering equation is:</p> \[C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \cdot \sigma(\mathbf{r}(t)) \cdot \mathbf{c}(\mathbf{r}(t), \mathbf{d}) \, dt\] <p>where $T(t) = \exp\left(-\int_{t_n}^{t} \sigma(\mathbf{r}(s)) \, ds\right)$ is the probability of a ray reaching point $t$ without hitting anything. For discrete samples, this becomes:</p> \[C(\mathbf{r}) = \sum_{i=1}^{N} T_i \cdot \alpha_i \cdot \mathbf{c}_i\] <p>where $\alpha_i = 1 - \exp(-\sigma_i \delta_i)$ is the probability of the ray terminating at sample $i$ with step size $\delta_i$, and $T_i = \prod_{j=1}^{i-1}(1 - \alpha_j)$ is the accumulated transmittance. I implemented this using <code class="language-plaintext highlighter-rouge">torch.cumprod</code> to compute the transmittance efficiently, allowing gradients to flow back through the rendering process during training.</p> <p>I trained the NeRF using Adam optimizer with learning rate $0.0005$, sampling 10,000 rays per batch for 5000 iterations. The model was optimized using MSE loss between rendered and ground truth colors, reaching over 23 PSNR on the validation set.</p> <div class="row"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part2/digger_psnr-480.webp 480w,/assets/img/cs180/p4/part2/digger_psnr-800.webp 800w,/assets/img/cs180/p4/part2/digger_psnr-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part2/digger_psnr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Wheel Loader Dataset MSR &amp; PSNR" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Below is the novel view synthesis on the wheel loader dataset.</p> <div class="row"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part2/first-480.webp 480w,/assets/img/cs180/p4/part2/first-800.webp 800w,/assets/img/cs180/p4/part2/first-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part2/first.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Wheel Loader iter=100" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part2/early-480.webp 480w,/assets/img/cs180/p4/part2/early-800.webp 800w,/assets/img/cs180/p4/part2/early-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part2/early.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Wheel Loader iter=300" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part2/1000-480.webp 480w,/assets/img/cs180/p4/part2/1000-800.webp 800w,/assets/img/cs180/p4/part2/1000-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part2/1000.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Wheel Loader iter=1000" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part2/3000-480.webp 480w,/assets/img/cs180/p4/part2/3000-800.webp 800w,/assets/img/cs180/p4/part2/3000-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part2/3000.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Wheel Loader iter=3000" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part2/final-480.webp 480w,/assets/img/cs180/p4/part2/final-800.webp 800w,/assets/img/cs180/p4/part2/final-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part2/final.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Wheel Loader iter=5000" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p class="text-center">iterations: 100, 300, 1000, 3000, 5000.</p> <h5 id="26-training-with-your-own-data">2.6: Training with your own data</h5> <p>For my own dataset, I trained a NeRF on the LEGO dataset collected earlier. To accommodate the real-world capture conditions, I adjusted several hyperparameters from the synthetic lego dataset. Most importantly, I tuned the near and far sampling bounds to 0.02 and 0.5 based on the actual distance between my camera and the object. I also increased the number of samples per ray from 32 to 64 for higher quality reconstruction, which increased training time but significantly improved detail.</p> <p>To generate novel views, I implemented a circular camera trajectory that orbits around the object while maintaining focus on the scene center using a <code class="language-plaintext highlighter-rouge">look_at_origin</code> function. I generated 60 frames by rotating the camera position around the object and rendering each view through the trained NeRF. The training process showed steady improvement in reconstruction quality, with the PSNR increasing as the network learned to represent the 3D structure and appearance.</p> <div class="row"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part2/lego_psnr-480.webp 480w,/assets/img/cs180/p4/part2/lego_psnr-800.webp 800w,/assets/img/cs180/p4/part2/lego_psnr-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part2/lego_psnr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lego Dataset MSR &amp; PSNR" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Below is the novel view synthesis on the wheel loader dataset.</p> <div class="row"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part2/orbit_iter_1000-480.webp 480w,/assets/img/cs180/p4/part2/orbit_iter_1000-800.webp 800w,/assets/img/cs180/p4/part2/orbit_iter_1000-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part2/orbit_iter_1000.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lego iter=1000" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part2/orbit_iter_5000-480.webp 480w,/assets/img/cs180/p4/part2/orbit_iter_5000-800.webp 800w,/assets/img/cs180/p4/part2/orbit_iter_5000-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part2/orbit_iter_5000.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lego iter=5000" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part2/orbit_final-480.webp 480w,/assets/img/cs180/p4/part2/orbit_final-800.webp 800w,/assets/img/cs180/p4/part2/orbit_final-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part2/orbit_final.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lego iter=10000" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p class="text-center">iterations: 1000, 5000, 10000.</p> <h6 id="bonus-another-nerf-render">Bonus: Another NeRF render!</h6> <div class="row"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cs180/p4/part2/lefufu-480.webp 480w,/assets/img/cs180/p4/part2/lefufu-800.webp 800w,/assets/img/cs180/p4/part2/lefufu-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cs180/p4/part2/lefufu.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="!" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Cyprian Zander. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>